{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMqxBUQ+9ALZJri87suCkNI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"n9IFkvtmpJit"},"outputs":[],"source":["#1 Mounting the Google Drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","data_root='/content/drive/My Drive/Colab Notebooks/Chatbot' #Please upload the files in your drive and change the path to it accordingly."]},{"cell_type":"code","source":["\n","import json\n","import string\n","import random\n","\n","import nltk\n","import numpy as np\n","from nltk.stem import WordNetLemmatizer\n","import tensorflow as tf\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","nltk.download(\"punkt\")\n","nltk.download(\"wordnet\")"],"metadata":{"id":"yKcZ4qwVpwSN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_file = open(data_root + '/intents.json').read()\n","data = json.loads(data_file)\n","\n","data"],"metadata":{"id":"XSIgqxFVp5-9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#4 Extracting data_X(features) and data_Y(Target)\n","\n","words = [] #For Bow model/ vocabulary for patterns\n","classes = [] #For Bow  model/ vocabulary for tags\n","data_X = [] #For storing each pattern\n","data_y = [] #For storing tag corresponding to each pattern in data_X\n","# Iterating over all the intents\n","\n","for intent in data[\"intents\"]:\n","    for pattern in intent[\"patterns\"]:\n","        tokens = nltk.word_tokenize(pattern) # tokenize each pattern\n","        words.extend(tokens) #and append tokens to words\n","        data_X.append(pattern) #appending pattern to data_X\n","        data_y.append(intent[\"tag\"]) ,# appending the associated tag to each pattern\n","\n","    # adding the tag to the classes if it's not there already\n","    if intent[\"tag\"] not in classes:\n","        classes.append(intent[\"tag\"])\n","\n","# initializing lemmatizer to get stem of words\n","lemmatizer = WordNetLemmatizer()\n","\n","# lemmatize all the words in the vocab and convert them to lowercase\n","# if the words don't appear in punctuation\n","words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in string.punctuation]\n","# sorting the vocab and classes in alphabetical order and taking the # set to ensure no duplicates occur\n","words = sorted(set(words))\n","classes = sorted(set(classes))"],"metadata":{"id":"vs7Ge0bBqE1s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 5 Text to Numbers\n","training = []\n","out_empty = [0] * len(classes)\n","# creating the bag of words model\n","for idx, doc in enumerate(data_X):\n","    bow = []\n","    text = lemmatizer.lemmatize(doc.lower())\n","    for word in words:\n","        bow.append(1) if word in text else bow.append(0)\n","    # mark the index of class that the current pattern is associated\n","    # to\n","    output_row = list(out_empty)\n","    output_row[classes.index(data_y[idx])] = 1\n","    # add the one hot encoded BoW and associated classes to training\n","    training.append([bow, output_row])\n","# shuffle the data and convert it to an array\n","random.shuffle(training)\n","training = np.array(training, dtype=object)\n","# split the features and target labels\n","train_X = np.array(list(training[:, 0]))\n","train_Y = np.array(list(training[:, 1]))"],"metadata":{"id":"jHOiDmi8qir_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#6 The Neural Network Model\n","model = Sequential()\n","model.add(Dense(128, input_shape=(len(train_X[0]),), activation=\"relu\"))\n","model.add(Dropout(0.5))\n","model.add(Dense(64, activation=\"relu\"))\n","model.add(Dropout(0.5))\n","model.add(Dense(len(train_Y[0]), activation = \"softmax\"))\n","adam = tf.keras.optimizers.Adam(learning_rate=0.01)\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=adam,\n","              metrics=[\"accuracy\"])\n","print(model.summary())\n","model.fit(x=train_X, y=train_Y, epochs=150, verbose=1)"],"metadata":{"id":"osrRhaqlqzyQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean_text(text):\n","  tokens = nltk.word_tokenize(text)\n","  tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","  return tokens\n","\n","def bag_of_words(text, vocab):\n","  tokens = clean_text(text)\n","  bow = [0] * len(vocab)\n","  for w in tokens:\n","    for idx, word in enumerate(vocab):\n","      if word == w:\n","        bow[idx] = 1\n","  return np.array(bow)\n","\n","def pred_class(text, vocab, labels):\n","  bow = bag_of_words(text, vocab)\n","  result = model.predict(np.array([bow]))[0] #Extracting probabilities\n","  thresh = 0.5\n","  y_pred = [[indx, res] for indx, res in enumerate(result) if res > thresh]\n","  y_pred.sort(key=lambda x: x[1], reverse=True) #Sorting by values of probability in decreasing order\n","  return_list = []\n","  for r in y_pred:\n","    return_list.append(labels[r[0]]) #Contains labels(tags) for highest probability\n","  return return_list\n","\n","def get_response(intents_list, intents_json):\n","  if len(intents_list) == 0:\n","    result = \"Sorry! I don't understand.\"\n","  else:\n","    tag = intents_list[0]\n","    list_of_intents = intents_json[\"intents\"]\n","    for i in list_of_intents:\n","      if i[\"tag\"] == tag:\n","        result = random.choice(i[\"responses\"])\n","        break\n","  return result"],"metadata":{"id":"lv7GwVm7tGer"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Interacting with the chatbot\n","print(\"Press 0 if you don't want to chat with our ChatBot.\")\n","while True:\n","    message = input(\"\")\n","    if message == \"0\":\n","      break\n","    intents = pred_class(message, words, classes)\n","    result = get_response(intents, data)\n","    print(result)"],"metadata":{"id":"W_RZ59IVtXwi"},"execution_count":null,"outputs":[]}]}